{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e24409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "class PricingDatasetGenerator:\n",
    "    \n",
    "    def __init__(self, device_catalog_path, defect_db_path):\n",
    "        with open(device_catalog_path, 'r') as f:\n",
    "            self.device_catalog = json.load(f)\n",
    "        \n",
    "        with open(defect_db_path, 'r') as f:\n",
    "            self.defect_db = json.load(f)['defects']\n",
    "        self.current_date = datetime(2025, 1, 10)\n",
    "        \n",
    "        print(f\"Loaded {len(self._get_all_devices())} device models\")\n",
    "        print(f\" Loaded {len(self.defect_db)} defect types\")\n",
    "    \n",
    "    def _get_all_devices(self):\n",
    "        devices = []\n",
    "        for brand_data in self.device_catalog['devices']:\n",
    "            brand = brand_data['brand']\n",
    "            for model in brand_data['models']:\n",
    "                devices.append({\n",
    "                    'brand': brand,\n",
    "                    'model': model['name'],\n",
    "                    'original_price': model['original_price'],\n",
    "                    'release_year': model['release_year']\n",
    "                })\n",
    "        return devices\n",
    "    \n",
    "    def _calculate_age_months(self, release_year, purchase_year):\n",
    "        purchase_date = datetime(purchase_year, random.randint(1, 12), 1)\n",
    "        age = (self.current_date - purchase_date).days / 30\n",
    "        return max(1, int(age))\n",
    "    \n",
    "    def _calculate_base_depreciation(self, original_price, age_months):\n",
    "        if age_months <= 12:\n",
    "            depreciation_rate = 0.15 * (age_months / 12)\n",
    "        else:\n",
    "            year_1_depreciation = 0.15\n",
    "            additional_years = (age_months - 12) / 12\n",
    "            additional_depreciation = additional_years * 0.10\n",
    "            depreciation_rate = year_1_depreciation + additional_depreciation\n",
    "        \n",
    "        depreciation_rate = min(depreciation_rate, 0.70)\n",
    "        \n",
    "        return original_price * (1 - depreciation_rate)\n",
    "    \n",
    "    def _select_defects(self):\n",
    "        if random.random() < 0.40:\n",
    "            return []\n",
    "        \n",
    "        if random.random() < 0.67:\n",
    "            return [random.choice(self.defect_db)]\n",
    "        \n",
    "        if random.random() < 0.75:\n",
    "            return random.sample(self.defect_db, 2)\n",
    "        \n",
    "        return random.sample(self.defect_db, random.randint(3, 4))\n",
    "    \n",
    "    def _calculate_condition_score(self, defects):\n",
    "        if not defects:\n",
    "            return random.uniform(9.0, 10.0)\n",
    "        \n",
    "        total_severity = sum(d['severity_score'] for d in defects)\n",
    "        avg_severity = total_severity / len(defects)\n",
    "        \n",
    "        condition = 10 - avg_severity\n",
    "        \n",
    "        condition += random.uniform(-0.5, 0.5)\n",
    "        \n",
    "        return max(0, min(10, condition))\n",
    "    \n",
    "    def _assign_condition_grade(self, condition_score):\n",
    "        if condition_score >= 9:\n",
    "            return 'A'\n",
    "        elif condition_score >= 7:\n",
    "            return 'B'\n",
    "        elif condition_score >= 5:\n",
    "            return 'C'\n",
    "        elif condition_score >= 3:\n",
    "            return 'D'\n",
    "        else:\n",
    "            return 'F'\n",
    "    \n",
    "    def _calculate_final_price(self, base_price, defects, condition_score):\n",
    "        price = base_price\n",
    "        \n",
    "        for defect in defects:\n",
    "            price *= (1 + defect['price_impact'])\n",
    "        \n",
    "        condition_factor = condition_score / 10\n",
    "        price *= (0.7 + 0.3 * condition_factor)\n",
    "        \n",
    "        noise = random.uniform(0.95, 1.05)\n",
    "        price *= noise\n",
    "        \n",
    "        price = round(price / 100) * 100\n",
    "        \n",
    "        return max(500, int(price))\n",
    "    \n",
    "    def generate_dataset(self, n_samples=2000):\n",
    "        \n",
    "        devices = self._get_all_devices()\n",
    "        data = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            device = random.choice(devices)\n",
    "            \n",
    "            years_old = random.randint(1, 4)\n",
    "            purchase_year = self.current_date.year - years_old\n",
    "            \n",
    "            age_months = self._calculate_age_months(\n",
    "                device['release_year'],\n",
    "                purchase_year\n",
    "            )\n",
    "            \n",
    "            base_price = self._calculate_base_depreciation(\n",
    "                device['original_price'],\n",
    "                age_months\n",
    "            )\n",
    "            \n",
    "            defects = self._select_defects()\n",
    "            \n",
    "            condition_score = self._calculate_condition_score(defects)\n",
    "            condition_grade = self._assign_condition_grade(condition_score)\n",
    "            \n",
    "            final_price = self._calculate_final_price(\n",
    "                base_price,\n",
    "                defects,\n",
    "                condition_score\n",
    "            )\n",
    "            \n",
    "            defect_ids = [d['id'] for d in defects]\n",
    "            defect_names = [d['name'] for d in defects]\n",
    "            \n",
    "            has_screen_damage = any(d['category'] == 'screen' for d in defects)\n",
    "            has_water_damage = any(d['category'] == 'water' for d in defects)\n",
    "            has_battery_issue = any(d['category'] == 'battery' for d in defects)\n",
    "            has_physical_damage = any(d['category'] == 'physical' for d in defects)\n",
    "            has_critical_defect = any(d['critical'] for d in defects)\n",
    "            \n",
    "            total_severity = sum(d['severity_score'] for d in defects)\n",
    "            avg_severity = total_severity / len(defects) if defects else 0\n",
    "            total_repair_cost = sum(d['repair_cost'] for d in defects)\n",
    "            \n",
    "            sample = {\n",
    "                'device_id': f\"DEV_{i:05d}\",\n",
    "                'brand': device['brand'],\n",
    "                'model': device['model'],\n",
    "                'original_price': device['original_price'],\n",
    "                'release_year': device['release_year'],\n",
    "                'purchase_year': purchase_year,\n",
    "                'age_months': age_months,\n",
    "                'num_defects': len(defects),\n",
    "                'defect_ids': ','.join(defect_ids) if defects else 'NONE',\n",
    "                'defect_names': ','.join(defect_names) if defects else 'NONE',\n",
    "                'has_screen_damage': int(has_screen_damage),\n",
    "                'has_water_damage': int(has_water_damage),\n",
    "                'has_battery_issue': int(has_battery_issue),\n",
    "                'has_physical_damage': int(has_physical_damage),\n",
    "                'has_critical_defect': int(has_critical_defect),\n",
    "                'total_severity_score': total_severity,\n",
    "                'avg_severity_score': round(avg_severity, 2),\n",
    "                'total_repair_cost': total_repair_cost,\n",
    "                'condition_score': round(condition_score, 2),\n",
    "                'condition_grade': condition_grade,\n",
    "                'base_price_after_depreciation': int(base_price),\n",
    "                'resale_price': final_price\n",
    "            }\n",
    "            \n",
    "            data.append(sample)\n",
    "            \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"  Generated {i + 1}/{n_samples} samples...\")\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        print(f\"\\nDataset generated!\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        print(f\"   Price range: ₹{df['resale_price'].min():,} - ₹{df['resale_price'].max():,}\")\n",
    "        print(f\"   Mean price: ₹{df['resale_price'].mean():,.0f}\")\n",
    "        print(f\"   Devices with defects: {(df['num_defects'] > 0).sum()} ({(df['num_defects'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generator = PricingDatasetGenerator(\n",
    "        device_catalog_path='data/device_catalog.json',\n",
    "        defect_db_path='data/defect_database.json'\n",
    "    )\n",
    "    \n",
    "    df = generator.generate_dataset(n_samples=2000)\n",
    "    \n",
    "    df.to_csv('data/pricing_dataset.csv', index=False)\n",
    "    print(f\"\\n Saved to data/pricing_dataset.csv\")\n",
    "    \n",
    "    print(\"\\nSample rows:\")\n",
    "    print(df[['brand', 'model', 'age_months', 'num_defects', \n",
    "              'condition_grade', 'resale_price']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "sys.path.append(\"./CLIP/clip\")\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "lmodel = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "defect_classes = [\n",
    "    \"cracked screen\",\n",
    "    \"screen scratch\",\n",
    "    \"broken hinge\",\n",
    "    \"keyboard malfunction\",\n",
    "    \"physical dent\",\n",
    "    \"water damage\",\n",
    "    \"charging port issue\",\n",
    "    \"normal device\",\n",
    "    \"battery drain\",\n",
    "    \"battery swelling\",\n",
    "    \"back panel damaging\",\n",
    "    \"overheating\",\n",
    "    \"audio issues\",\n",
    "    \"camera defect\",\n",
    "    \"button damage\",\n",
    "    \"display flickering\"\n",
    "]\n",
    "text_tokens = clip.tokenize(defect_classes).to(device)\n",
    "def clip_predict(image_path):\n",
    "    image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        probs = (image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    idx = probs.argmax().item()\n",
    "    return defect_classes[idx], probs[0][idx].item()\n",
    "def llava_caption(image_path, defect_label):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except:\n",
    "        return f\"The device shows {defect_label} with no other visible defects.\"\n",
    "\n",
    "    prompt = f\"\"\"<image>\n",
    "Describe this device's defected part of the image clearly for training a machine learning model.\n",
    "The defect type is: {defect_label}.\n",
    "Be concise and factual.\n",
    "\"\"\"\n",
    "\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(lmodel.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = lmodel.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    full_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "    caption = full_text.split(\"Be concise and factual.\")[-1].strip()\n",
    "    return caption\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(image_dir, output_csv):\n",
    "    dataset = []\n",
    "\n",
    "    for img in tqdm(os.listdir(image_dir)):\n",
    "        if not img.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(image_dir, img)\n",
    "\n",
    "        label, conf = clip_predict(path)\n",
    "        caption = caption = llava_caption(path, label)\n",
    "\n",
    "\n",
    "        dataset.append({\n",
    "            \"image_path\": path,\n",
    "            \"device_type\": \"phone\",\n",
    "            \"defect_type\": label,\n",
    "            \"text_prompt\": caption\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(dataset).to_csv(output_csv, index=False)\n",
    "    print(f\"Saved to {output_csv}\")\n",
    "\n",
    "build_dataset(\n",
    "    image_dir=\"/kaggle/input/phone-s/phone_screen_defects\",\n",
    "    output_csv=\"training_data_phone2.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3afcc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path device_type  \\\n",
      "0  data\\Laptop-damage-detection-testing.v1i.coco\\...      laptop   \n",
      "1  data\\Laptop-damage-detection-testing.v1i.coco\\...      laptop   \n",
      "2  data\\Laptop-damage-detection-testing.v1i.coco\\...      laptop   \n",
      "3  data\\Laptop-damage-detection-testing.v1i.coco\\...      laptop   \n",
      "4  data\\Laptop-damage-detection-testing.v1i.coco\\...      laptop   \n",
      "\n",
      "           defect_type                                        text_prompt  \n",
      "0   display flickering  The image is blurry and has a flickering display.  \n",
      "1  back panel damaging           The back panel of the laptop is damaged.  \n",
      "2   display flickering  The image shows a display with a flickering sc...  \n",
      "3  back panel damaging           The back panel of the device is damaged.  \n",
      "4  back panel damaging           The back panel of the laptop is damaged.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(r\"E:\\fortransferee\\mlproject7\\aws_cust\\data\\laptop\\training_data_laptop21.csv\")\n",
    "\n",
    "df[\"image_path\"] = df[\"image_path\"].apply(\n",
    "    lambda x: os.path.join(\n",
    "        \"data\\Laptop-damage-detection-testing.v1i.coco\",\n",
    "        os.path.basename(x)\n",
    "    )\n",
    ")\n",
    "\n",
    "df.to_csv(\"training_data_laptop213.csv\", index=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class CLIPEvaluator:\n",
    "    \"\"\"Evaluate and compare CLIP models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def load_model(self, model_path, model_name=\"Model\"):\n",
    "        \"\"\"Load a CLIP model\"\"\"\n",
    "        print(f\"\\nLoading {model_name}...\")\n",
    "        print(f\"   Path: {model_path}\")\n",
    "        \n",
    "        try:\n",
    "            model = CLIPModel.from_pretrained(model_path)\n",
    "            processor = CLIPProcessor.from_pretrained(model_path)\n",
    "            model.eval()\n",
    "            print(f\" {model_name} loaded successfully\")\n",
    "            return model, processor\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {model_name}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def prepare_test_data(self, csv_path, max_samples=None):\n",
    "        \"\"\"Load test dataset\"\"\"\n",
    "        print(f\"\\nLoading test data from {csv_path}...\")\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        if max_samples:\n",
    "            df = df.sample(n=min(max_samples, len(df)), random_state=42)\n",
    "        \n",
    "        valid_rows = []\n",
    "        for idx, row in df.iterrows():\n",
    "            if os.path.exists(row['image_path']):\n",
    "                valid_rows.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(valid_rows)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} test samples\")\n",
    "        print(f\"   Unique defects: {df['defect_type'].nunique()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def predict_batch(self, model, processor, images, texts, device='cpu'):\n",
    "       \n",
    "        model = model.to(device)\n",
    "        \n",
    "        inputs = processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            probs = logits_per_image.softmax(dim=1)\n",
    "        \n",
    "        return probs.cpu().numpy()\n",
    "    \n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model,\n",
    "        processor,\n",
    "        test_df,\n",
    "        model_name=\"Model\",\n",
    "        batch_size=32,\n",
    "        device='cpu'\n",
    "    ):\n",
    "       \n",
    "        print(f\"\\n Evaluating {model_name}...\")\n",
    "        \n",
    "        unique_defects = sorted(test_df['defect_type'].unique())\n",
    "        defect_to_idx = {defect: idx for idx, defect in enumerate(unique_defects)}\n",
    "        \n",
    "        print(f\"   Defect classes: {len(unique_defects)}\")\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        all_confidences = []\n",
    "        \n",
    "        for start_idx in tqdm(range(0, len(test_df), batch_size), desc=f\"Evaluating {model_name}\"):\n",
    "            batch_df = test_df.iloc[start_idx:start_idx + batch_size]\n",
    "            \n",
    "            images = []\n",
    "            texts = []\n",
    "            true_labels = []\n",
    "            \n",
    "            for _, row in batch_df.iterrows():\n",
    "                try:\n",
    "                    img = Image.open(row['image_path']).convert('RGB')\n",
    "                    img.load()\n",
    "                    images.append(img)\n",
    "                    texts.append(str(row['text_prompt']))\n",
    "                    true_labels.append(defect_to_idx[row['defect_type']])\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            if not images:\n",
    "                continue\n",
    "            \n",
    "            defect_descriptions = [str(row['text_prompt']) for _, row in test_df.iterrows()]\n",
    "            defect_descriptions = list(set(defect_descriptions))\n",
    "            \n",
    "            for img, true_label in zip(images, true_labels):\n",
    "                probs = self.predict_batch(\n",
    "                    model,\n",
    "                    processor,\n",
    "                    [img] * len(unique_defects),\n",
    "                    [test_df[test_df['defect_type'] == defect]['text_prompt'].iloc[0] \n",
    "                     for defect in unique_defects],\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                pred_idx = np.argmax(np.diag(probs))\n",
    "                confidence = np.max(np.diag(probs))\n",
    "                \n",
    "                all_predictions.append(pred_idx)\n",
    "                all_true_labels.append(true_label)\n",
    "                all_confidences.append(confidence)\n",
    "        \n",
    "        accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            all_true_labels,\n",
    "            all_predictions,\n",
    "            average='weighted',\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        per_class_precision, per_class_recall, per_class_f1, support = precision_recall_fscore_support(\n",
    "            all_true_labels,\n",
    "            all_predictions,\n",
    "            average=None,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "        \n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'avg_confidence': np.mean(all_confidences),\n",
    "            'predictions': all_predictions,\n",
    "            'true_labels': all_true_labels,\n",
    "            'confidences': all_confidences,\n",
    "            'confusion_matrix': cm,\n",
    "            'per_class_metrics': {\n",
    "                'defects': unique_defects,\n",
    "                'precision': per_class_precision.tolist(),\n",
    "                'recall': per_class_recall.tolist(),\n",
    "                'f1': per_class_f1.tolist(),\n",
    "                'support': support.tolist()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"   Accuracy:       {accuracy:.2%}\")\n",
    "        print(f\"   Precision:      {precision:.2%}\")\n",
    "        print(f\"   Recall:         {recall:.2%}\")\n",
    "        print(f\"   F1-Score:       {f1:.2%}\")\n",
    "        print(f\"   Avg Confidence: {np.mean(all_confidences):.2%}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compare_models(self, results_pretrained, results_finetuned, save_dir='evaluation'):\n",
    "        \"\"\"Compare two models and generate visualizations\"\"\"\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        \n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1', 'avg_confidence']\n",
    "        \n",
    "        print(f\"\\n{'Metric':<20} {'Pre-trained':<15} {'Fine-tuned':<15} {'Improvement':<15}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        improvements = {}\n",
    "        for metric in metrics:\n",
    "            pretrained_val = results_pretrained[metric]\n",
    "            finetuned_val = results_finetuned[metric]\n",
    "            improvement = ((finetuned_val - pretrained_val) / pretrained_val) * 100\n",
    "            improvements[metric] = improvement\n",
    "            \n",
    "            print(f\"{metric.replace('_', ' ').title():<20} \"\n",
    "                  f\"{pretrained_val:>13.2%}  \"\n",
    "                  f\"{finetuned_val:>13.2%}  \"\n",
    "                  f\"{improvement:>+13.1f}%\")\n",
    "        \n",
    "        comparison = {\n",
    "            'pre_trained': {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "                           for k, v in results_pretrained.items() \n",
    "                           if k not in ['predictions', 'true_labels', 'confidences', 'confusion_matrix']},\n",
    "            'fine_tuned': {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "                          for k, v in results_finetuned.items() \n",
    "                          if k not in ['predictions', 'true_labels', 'confidences', 'confusion_matrix']},\n",
    "            'improvements': {k: float(v) for k, v in improvements.items()}\n",
    "        }\n",
    "        \n",
    "        with open(f'{save_dir}/comparison.json', 'w') as f:\n",
    "            json.dump(comparison, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n Comparison saved to {save_dir}/comparison.json\")\n",
    "        \n",
    "        self.plot_comparison(results_pretrained, results_finetuned, save_dir)\n",
    "        \n",
    "        return improvements\n",
    "    \n",
    "    def plot_comparison(self, results_pretrained, results_finetuned, save_dir):\n",
    "        \"\"\"Generate comparison plots\"\"\"\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        pretrained_vals = [results_pretrained[m] for m in metrics]\n",
    "        finetuned_vals = [results_finetuned[m] for m in metrics]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, pretrained_vals, width, label='Pre-trained', color='#3498db')\n",
    "        bars2 = ax.bar(x + width/2, finetuned_vals, width, label='Fine-tuned', color='#2ecc71')\n",
    "        \n",
    "        ax.set_ylabel('Score', fontsize=12)\n",
    "        ax.set_title('Pre-trained vs Fine-tuned CLIP Performance', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.set_ylim([0, 1])\n",
    "        \n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.1%}',\n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_dir}/metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\" Saved metrics comparison to {save_dir}/metrics_comparison.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        cm1 = results_pretrained['confusion_matrix']\n",
    "        sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar=False)\n",
    "        ax1.set_title('Pre-trained CLIP\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('True Label')\n",
    "        ax1.set_xlabel('Predicted Label')\n",
    "        \n",
    "        cm2 = results_finetuned['confusion_matrix']\n",
    "        sns.heatmap(cm2, annot=True, fmt='d', cmap='Greens', ax=ax2, cbar=False)\n",
    "        ax2.set_title('Fine-tuned CLIP\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('True Label')\n",
    "        ax2.set_xlabel('Predicted Label')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_dir}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved confusion matrices to {save_dir}/confusion_matrices.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax.hist(results_pretrained['confidences'], bins=30, alpha=0.5, \n",
    "               label='Pre-trained', color='#3498db', edgecolor='black')\n",
    "        ax.hist(results_finetuned['confidences'], bins=30, alpha=0.5, \n",
    "               label='Fine-tuned', color='#2ecc71', edgecolor='black')\n",
    "        \n",
    "        ax.set_xlabel('Confidence Score', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_dir}/confidence_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\" Saved confidence distribution to {save_dir}/confidence_distribution.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(\"CLIP MODEL EVALUATION: PRE-TRAINED VS FINE-TUNED\")\n",
    "\n",
    "    \n",
    "    CONFIG = {\n",
    "        'pretrained_model': 'openai/clip-vit-base-patch32',\n",
    "        'finetuned_model': 'models/finetuned_clip/best_model',\n",
    "        'test_csv_phone': 'data/splits/val.csv',\n",
    "        'test_csv_laptop': 'data/splits/val.csv',\n",
    "        'batch_size': 16,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'max_test_samples': 500\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    for key, value in CONFIG.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    evaluator = CLIPEvaluator()\n",
    "    \n",
    "    pretrained_model, pretrained_processor = evaluator.load_model(\n",
    "        CONFIG['pretrained_model'],\n",
    "        \"Pre-trained CLIP\"\n",
    "    )\n",
    "    \n",
    "    finetuned_model, finetuned_processor = evaluator.load_model(\n",
    "        CONFIG['finetuned_model'],\n",
    "        \"Fine-tuned CLIP\"\n",
    "    )\n",
    "    \n",
    "    if not finetuned_model:\n",
    "        print(\"\\nFine-tuned model not found!\")\n",
    "        print(\"   Please train the model first using: python src/finetune_clip.py\")\n",
    "        return\n",
    "    \n",
    "    test_df = evaluator.prepare_test_data(\n",
    "        CONFIG['test_csv_phone'],\n",
    "        max_samples=CONFIG['max_test_samples']\n",
    "    )\n",
    "    \n",
    "    results_pretrained = evaluator.evaluate_model(\n",
    "        pretrained_model,\n",
    "        pretrained_processor,\n",
    "        test_df,\n",
    "        model_name=\"Pre-trained CLIP\",\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    results_finetuned = evaluator.evaluate_model(\n",
    "        finetuned_model,\n",
    "        finetuned_processor,\n",
    "        test_df,\n",
    "        model_name=\"Fine-tuned CLIP\",\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    improvements = evaluator.compare_models(\n",
    "        results_pretrained,\n",
    "        results_finetuned,\n",
    "        save_dir='evaluation'\n",
    "    )\n",
    "    \n",
    "    print(\"EVALUATION COMPLETE\")\n",
    "    print(f\"\\n Results saved to: evaluation/\")\n",
    "    print(f\"   • comparison.json\")\n",
    "    print(f\"   • metrics_comparison.png\")\n",
    "    print(f\"   • confusion_matrices.png\")\n",
    "    print(f\"   • confidence_distribution.png\")\n",
    "    \n",
    "    print(f\" KEY IMPROVEMENT:\")\n",
    "    print(f\"   Accuracy: {results_pretrained['accuracy']:.2%} → {results_finetuned['accuracy']:.2%} \"\n",
    "          f\"({improvements['accuracy']:+.1f}%)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19412ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Analysis:\n",
      "Total samples: 209\n",
      "\n",
      "Defect type distribution:\n",
      "defect_type\n",
      "cracked screen         176\n",
      "back panel damaging     18\n",
      "battery swelling         9\n",
      "charging port issue      2\n",
      "normal device            2\n",
      "camera defect            1\n",
      "broken hinge             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Device type distribution:\n",
      "device_type\n",
      "phone    209\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique defects: 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_df = pd.read_csv(r'E:\\fortransferee\\mlproject7\\aws_cust\\data\\phone\\phone_training_data2134.csv')\n",
    "\n",
    "print(\"Validation Set Analysis:\")\n",
    "print(f\"Total samples: {len(val_df)}\")\n",
    "print(f\"\\nDefect type distribution:\")\n",
    "print(val_df['defect_type'].value_counts())\n",
    "print(f\"\\nDevice type distribution:\")\n",
    "print(val_df['device_type'].value_counts())\n",
    "print(f\"\\nUnique defects: {val_df['defect_type'].nunique()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
